{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModellingTK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M0d5YdaHbXw",
        "outputId": "7e55304f-4c40-4bba-db54-a3265e87625f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.3)\n",
            "Requirement already satisfied: fuzzymatcher in /usr/local/lib/python3.7/dist-packages (0.0.5)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (from fuzzymatcher) (0.12.2)\n",
            "Requirement already satisfied: metaphone in /usr/local/lib/python3.7/dist-packages (from fuzzymatcher) (0.6)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from fuzzymatcher) (2.8.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fuzzymatcher) (1.3.5)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.7/dist-packages (from fuzzymatcher) (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fuzzymatcher) (1.21.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fuzzymatcher) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->fuzzymatcher) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein->fuzzymatcher) (57.4.0)\n"
          ]
        }
      ],
      "source": [
        "# Import section\n",
        "!pip install unidecode\n",
        "!pip install fuzzymatcher\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers, metrics\n",
        "from tensorflow.keras.layers import Normalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "from datetime import date, timedelta\n",
        "from clean import clean_all, get_data, get_bairros_data\n",
        "from preproc import get_format, extract_ts\n",
        "idx = pd.IndexSlice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_format(data):\n",
        "    '''Function taking a clean dataset and returning 2 ARIMA-friendly df:\n",
        "    1. total nb_crime / day and AR\n",
        "    2. nb_crime per 1000 inhab / day and AR'''\n",
        "\n",
        "    data[\"Date\"] = data[\"Date_Time\"].dt.date\n",
        "    preprocessed_data = data.groupby([\"AR\", \"Date\"]).count()[[\"Crime_ID\"]]\n",
        "    preprocessed_data.rename(columns={\"Crime_ID\":\"nb_crimes\"}, inplace=True)\n",
        "\n",
        "    ## add a column with yearly population per AR\n",
        "    pop_clean = clean_pop_data()\n",
        "    preprocessed_data[\"year_temp\"] = preprocessed_data.index.map(lambda x: x[1].year)\n",
        "    input_merge = preprocessed_data.reset_index()\n",
        "    data_merge = pd.merge(left=input_merge, right=pop_clean, left_on=[\"AR\",\"year_temp\"],\\\n",
        "        right_on=[\"administrative_regions\", \"Ano\"])\n",
        "    data_merge.drop(columns=[\"administrative_regions\",\"Ano\"], inplace = True)\n",
        "    preprocessed_data.drop(columns=[\"year_temp\"],inplace = True)\n",
        "    ## compute nb crimes / 1000 inhabitants\n",
        "    data_merge[\"nb_crimes_1000\"]=data_merge.nb_crimes / data_merge.Populacao*1000\n",
        "    data_merge.drop(columns=[\"nb_crimes\",\"Populacao\",\"year_temp\"], inplace=True)\n",
        "    data_merge.rename(columns={'nb_crimes_1000': 'nb_crimes'},inplace=True)\n",
        "    data_merge.set_index([\"AR\",\"Date\"],inplace=True)\n",
        "    ##\n",
        "    preprocessed_data = preprocessed_data.unstack(level=0)\n",
        "    preprocessed_data = preprocessed_data.replace(np.nan, 0).astype(int)\n",
        "    preprocessed_data_1000 = data_merge.unstack(level=0)\n",
        "    preprocessed_data_1000 = preprocessed_data_1000.replace(np.nan, 0)\n",
        "    return preprocessed_data, preprocessed_data_1000\n",
        "\n",
        "def get_popfile():\n",
        "    '''import population file'''\n",
        "    return pd.read_csv(\"population_Rio.csv\", sep=\",\")\n",
        "\n",
        "def clean_pop_data():\n",
        "    data= get_popfile()\n",
        "    data = data.drop(columns=[\"DensidadeBruta\", \"DensidadeLiquida\", \"TaxaGeometrica\"])\n",
        "\n",
        "    # Get rid of Roman numbers in front of name of administrative regions\n",
        "    splitted_regions = data['RegiaoAdministrativa'].str.split().str[1:]\n",
        "    cleaned_regions = splitted_regions.str.join(\" \")\n",
        "    data[\"administrative_regions\"] = cleaned_regions\n",
        "    data = data.drop(columns=[\"RegiaoAdministrativa\"])\n",
        "\n",
        "    # Dictionary of population data per region for 2000-2020 in 5-years steps\n",
        "    regions_dict = {}\n",
        "    for region in data[\"administrative_regions\"].unique():\n",
        "        regions_dict[region] = data[data[\"administrative_regions\"]==region]\n",
        "\n",
        "    # Dataframe with years for 2000-2020 in 1-year steps\n",
        "    year_df = pd.DataFrame(pd.period_range(min(data.Ano), max(data.Ano), freq=\"Y\"), columns=[\"Ano\"])\n",
        "    year_df = year_df[[\"Ano\"]].astype(\"str\").astype(\"int64\")\n",
        "\n",
        "    # Extend time series to annual time series and interpolate lineraly the missing population data\n",
        "    middle_dict = {}\n",
        "    for k, v in regions_dict.items():\n",
        "        middle_dict[k] = year_df.merge(v, how=\"left\", on=\"Ano\")\n",
        "        middle_dict[k][\"administrative_regions\"].fillna(value=k, inplace=True)\n",
        "        middle_dict[k][\"Populacao\"].interpolate(method='linear', inplace=True)\n",
        "\n",
        "\n",
        "    # Create a new dataframe with cleand and extended population data\n",
        "    empty = pd.DataFrame(columns=[\"Ano\", \"Populacao\", \"administrative_regions\"])\n",
        "    df = pd.concat(middle_dict).reset_index().drop(columns=[\"level_0\", \"level_1\"])\n",
        "\n",
        "    return df\n",
        "\n",
        "def extract_ts(df, AR):\n",
        "    '''Extract the time series from selected df and for each AR'''\n",
        "    df1 = df.reset_index()\n",
        "    df2 = df1[[(     'Date',                   ''), ('nb_crimes',AR)]]\n",
        "    df2.columns = df2.columns.droplevel()\n",
        "    df2.columns=[\"ds\", \"y\"]\n",
        "    df2.ds = df2.ds.map(pd.to_datetime)\n",
        "    return df2\n"
      ],
      "metadata": {
        "id": "udEHH9hBiiaR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading\n",
        "data1 = pd.read_csv('parte1.csv', sep=';', encoding = 'iso-8859-1')\n",
        "data2 = pd.read_csv('parte2.csv', sep=';', encoding = 'iso-8859-1')\n",
        "data3 = pd.read_csv('parte3.csv', sep=';', encoding = 'iso-8859-1')\n",
        "data4 = pd.read_csv('parte4.csv', sep=';', encoding = 'iso-8859-1')\n",
        "bairros = pd.read_csv('bairros_lista.csv', encoding = 'iso-8859-1')\n",
        "data = clean_all(data1, data2, data3, data4, bairros)\n",
        "preprocessed_data, preprocessed_data_1000 = get_format(data)\n",
        "pop_clean = clean_pop_data()"
      ],
      "metadata": {
        "id": "e_85dn3tHg-d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing data\n",
        "def subsample_sequence(data, length, AR): # Return a shorter dataframe with specified \n",
        "                                                 # length for a specific barrio\n",
        "    last_possible = data.shape[0] - length\n",
        "    random_start = np.random.randint(0, last_possible)\n",
        "    data_sample = data[random_start: random_start+length]\n",
        "    data_sample = data_sample.loc[:,idx[:,AR]]\n",
        "\n",
        "    return data_sample\n",
        "\n",
        "def split_subsample_sequence(data, length, AR): # Return a random sequence of specified length\n",
        "\n",
        "    data_subsample = subsample_sequence(data, length, AR)\n",
        "    y_sample = data_subsample.iloc[length-31:]\n",
        "    \n",
        "    X_sample = data_subsample[0:length-31]\n",
        "    X_sample = X_sample.values\n",
        "    return np.array(X_sample), np.array(y_sample)\n",
        "\n",
        "def get_X_y(data, n_sequences, length, AR): # Return a sepcific number of (X,y) samples of specified length\n",
        "                                                   # for a specified bairro\n",
        "\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(n_sequences):\n",
        "        (xi, yi) = split_subsample_sequence(data, length, AR)\n",
        "        X.append(xi)\n",
        "        y.append(yi)\n",
        "        \n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "def get_train_test(data,n_sequences,length, AR): # Return train and test data\n",
        "\n",
        "    len_ = int(0.8*data.shape[0])\n",
        "    data_train = data[:len_]\n",
        "    data_test = data[len_:]\n",
        "    \n",
        "    test_seq = math.floor(n_sequences/4)\n",
        "    \n",
        "    X_train, y_train = get_X_y(data_train, n_sequences, length, AR)\n",
        "    X_test, y_test = get_X_y(data_test, test_seq, length, AR)\n",
        "    \n",
        "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1],1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1],1)\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "metadata": {
        "id": "flthJ-f4RlEp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Loading X_train/test, y_train/test for each AR in a global dictionnary\n",
        "\n",
        "AR_list = preprocessed_data_1000.columns.levels[1].tolist()\n",
        "all_data = {}\n",
        "\n",
        "for AR in AR_list:\n",
        "    data = get_train_test(preprocessed_data_1000, 500, 120, AR)\n",
        "    all_data[AR] = data"
      ],
      "metadata": {
        "id": "otFg5GQnek_a"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outside preprocessing \n",
        "\n",
        "for AR in AR_list:\n",
        "    normalizer = Normalization()\n",
        "    normalizer.adapt(all_data[AR][0])\n",
        "    normalizer.adapt(all_data[AR][2])"
      ],
      "metadata": {
        "id": "pkQ5guRwemjn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a model\n",
        "def init_model(AR):\n",
        "    metric = metrics.MAPE\n",
        "    opt = optimizers.RMSprop(learning_rate=0.005)\n",
        "    \n",
        "    model = models.Sequential()\n",
        "    model.add(layers.LSTM(30, return_sequences=True, activation='tanh'))\n",
        "    model.add(layers.LSTM(10, activation='tanh'))\n",
        "    model.add(layers.Dense(5, activation='relu'))\n",
        "    model.add(layers.Dense(31, activation='linear'))\n",
        "    \n",
        "    model.compile(loss='mse', \n",
        "                  optimizer=opt, \n",
        "                  metrics=[metric])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Parameters\n",
        "# def get_parameters(): \n",
        "es = EarlyStopping(monitor='val_loss', verbose=1, patience=10, restore_best_weights=True)\n",
        "\n",
        "# Fitting the model\n",
        "def fit(model, AR):\n",
        "    hist = model.fit(all_data[AR][0], all_data[AR][1],\n",
        "            validation_split=0.3,\n",
        "            epochs=20, \n",
        "            batch_size=32,\n",
        "            callbacks=[es], verbose=0)\n",
        "    return hist\n",
        "\n",
        "# Evaluating the model \n",
        "def evaluate(hist, AR):\n",
        "    results = hist.model.evaluate(all_data[AR][2], all_data[AR][3])\n",
        "    return results\n",
        "\n",
        "# All-in-1\n",
        "def modelling(AR):\n",
        "    results = evaluate(fit(init_model(AR), AR), AR)\n",
        "    return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "lCbjrOuReruf",
        "outputId": "b0d77399-b576-4cee-f63d-d206bdd23af6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d11a66b8684a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# def get_parameters():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Fitting the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EarlyStopping' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the results for each AR\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for AR in AR_list:\n",
        "    all_results[AR] = modelling(AR)\n",
        "    \n",
        "all_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IdEMNO5euKj",
        "outputId": "73d0b6e6-7ca9-4c43-ce24-64aac7a14d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restoring model weights from the end of the best epoch: 7.\n",
            "Epoch 17: early stopping\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0012 - mean_absolute_percentage_error: 36.5000\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "Epoch 11: early stopping\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 3.1746e-04 - mean_absolute_percentage_error: 28.4784\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "Epoch 15: early stopping\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 6.5810e-04 - mean_absolute_percentage_error: 43.0536\n",
            "Restoring model weights from the end of the best epoch: 6.\n",
            "Epoch 16: early stopping\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0034 - mean_absolute_percentage_error: 31.1262\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "Epoch 11: early stopping\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 5.5708e-05 - mean_absolute_percentage_error: 28.0939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the mean of all the results\n",
        "mape = [all_results[AR][1] for AR in all_results]\n",
        "mape_overall = np.mean(mape)\n",
        "mape_overall"
      ],
      "metadata": {
        "id": "e5Uw8uoXeweH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}